{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c037a62d",
   "metadata": {},
   "source": [
    "## Morning practical 1 day 2\n",
    "\n",
    "Hi there. Here, you're going to implement the sigmoid function and the cost function for logistic regression, before running the algorithm on a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to set things up\n",
    "import ipywidgets as widgets, numpy as np, pandas as pd\n",
    "from numpy.random import default_rng\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7131a7e",
   "metadata": {},
   "source": [
    "## Making and plotting the sigmoid function\n",
    "\n",
    "As a first exercise, do the following:\n",
    "* implement the sigmoid function. Note that $e^x$ is `np.exp(x)` in Numpy.\n",
    "* get 100 numbers from a normal distribution `rng = default_rng() ; vals = rng.standard_normal(100)`\n",
    "* apply your sigmoid function to these 100 values\n",
    "* plot the sigmoid outputs on the y axis and the original values on the x-axis\n",
    "\n",
    "Plotting hints:\n",
    "* start a new plot with `fig, ax = plt.subplots()`\n",
    "* you can plot points with `ax.scatter(xData, yData, c= \"red\", label = \"WowCoolMan\")`\n",
    "* you can label the x- and y-axis with `ax.set_xlabel(\"Banana\"); ax.set_ylabel(\"Happiness level\")`\n",
    "* add a legend with `ax.legend()` (wait for it ....ary)\n",
    "* once you are done plotting, you need to run `fig.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600538de",
   "metadata": {},
   "outputs": [],
   "source": [
    "testArray = np.array([-50, -1, 0, 1, 50])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f75dcd",
   "metadata": {},
   "source": [
    "## Assigning a class\n",
    "\n",
    "Let's have these random numbers stand in for real output from logistic regression. Say we want to classify samples as 1 if they have >= 70% chance of being positive, whereas we want to classify all other samples as negative. Up to you to make it happen!\n",
    "\n",
    "* Get an array of predicted class membership based on your Sigmoid function output. Call it `predictedClass`. For example, you could make a vector of zeros with `np.zeros`, and then set the elements that are >= 0.7 in your sigmoid output to 1. Or you could use `np.where` with x and y arguments (see [here](https://numpy.org/doc/stable/reference/generated/numpy.where.html)). \n",
    "* Change your plotting code to:\n",
    "    * colour the points by their assigned class (use two `ax.scatter` calls with different labels)\n",
    "    * add a vertical dashed line where we've placed our threshold (you'll probably need [this](https://stackoverflow.com/a/68087129), and `ax.vlines` [(click)](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.vlines.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef907c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9a03b26",
   "metadata": {},
   "source": [
    "## Running logistic regression\n",
    "\n",
    "To run logistic regression, we'll need to do a few things:\n",
    "* Make a function called `linAlgLogRegHypothesis(x, thetas)` that internally runs `linAlgRegHypothesis` and then uses the sigmoid function on those outputs and returns this.\n",
    "* Define a new cost function `costFuncLogReg(x, y, thetas)`.\n",
    "* Change gradient descent to allow it to select the hypothesis function it uses.\n",
    "\n",
    "First make `linAlgLogRegHypothesis`. Remember, this is just a function that calls `mySigmoid(linAlgRegHypothesis(data, thetas))` and returns the result. <br> <br> \n",
    "\n",
    "**Hint:**\n",
    "* The output of your `linAlgLogRegHypothesis` using `testX` as data and `testThetas` as parameters should be: <br>\n",
    "`[[0.71300016]\n",
    " [0.95302385]]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.array([[0.2, -0.03], [-0.3, 0.52]])\n",
    "testY = np.array([[0], [1]])\n",
    "testThetas = np.array([[0.8], [1.3], [5]])\n",
    "\n",
    "# note that this function automatically appends the column of ones, so you don't need to do that yourself!\n",
    "def linAlgRegHypothesis(data, thetas):\n",
    "    data = np.array(data)\n",
    "    oneFeatToAdd = np.ones(len(data))\n",
    "    newFeatArray = np.c_[oneFeatToAdd, data]\n",
    "    #make sure thetas are always of the form np.array([[theta1], [theta2]]), i.e. column vector\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    predictions = newFeatArray @ thetas\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b02b0be",
   "metadata": {},
   "source": [
    "## Making the cost function\n",
    "\n",
    "Now, make a new cost function `costFuncLogReg(x, y, thetas)`. It should implement the formula: <br> <br>$$Cost(x) = -y \\cdot log(h_\\theta(x))- (1-y) \\cdot log(1-h_\\theta(x))$$ <br> with $$h_\\theta(x) = sigmoid(\\theta^T \\cdot x)$$\n",
    "<br>\n",
    "Note that it should work with arrays, i.e. y is a column vector of 0 or 1, which looks like this: $y = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\end{bmatrix}$, and x is an array with samples on the rows and their features in the columns (such as $ X = \\begin{bmatrix} 0.18 & 1.03 \\\\ \\vdots & \\vdots \\\\ -0.72 & 0.4 \\end{bmatrix}$. <br> Use `np.nansum` rather than `np.sum`: this ignores NaNs if for some reason they are present in the data, rather than throwing an error. \n",
    "\n",
    "**Hint:**\n",
    "* The output of `costFuncLogReg(testX, testY, testThetas)` should be ~`0.64819`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991387cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ba7e8c",
   "metadata": {},
   "source": [
    "## Using gradient descent for your logistic regression\n",
    "\n",
    "Nearly there, we just need to change the gradient descent function from yesterday to use logistic regression for prediction rather than linear regression. We'll do that by adding an argument `cost_fun` which can be either `'linear'` or `'logistic'`. <br> A copy of the gradient descent function from yesterday is provided below. Your job is to add an extra argument.\n",
    "\n",
    "* Your function definition should look like `def linAlgGradientDescent(x, y, thetas, alpha, cost_fun = \"linear\"):`\n",
    "* Make sure to define the appropriate checks with if-statements.\n",
    "\n",
    "\n",
    "\n",
    "**Optional:** here's a link to a derivation showing that the gradient of the logistic regression cost function is the same as that of linear regression, and that it is convex: [LogRegGradientDerivation](WhyLogRegGradEqual.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linAlgGradientDescent(x, y, thetas, alpha) :\n",
    "    m = len(x)\n",
    "    if thetas.ndim < 2:\n",
    "        thetas = thetas[:, np.newaxis]\n",
    "    preds  = linAlgRegHypothesis(x, thetas)\n",
    "    if preds.shape != (m, 1):\n",
    "        preds  = preds[:, np.newaxis]\n",
    "    if y.ndim < 2:\n",
    "        y = y[:, np.newaxis]\n",
    "    errors = preds - y\n",
    "    gradientSummation  = errors.T @ np.c_[np.ones(len(errors)), x]\n",
    "    finalGradientSteps = alpha/m * gradientSummation\n",
    "    newThetas          = thetas - finalGradientSteps.T\n",
    "    return newThetas\n",
    "\n",
    "# your answer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "080093d9",
   "metadata": {},
   "source": [
    "## Testing your gradient descent function\n",
    "\n",
    "The below puts your function to the test. We assume that the predictedClass you made earlier (where you used 100 random normally distributed values, put them into the sigmoid, and assigned the 20% highest values as 1 and the rest as 0) are the true labels. We start from random thetas, update them by gradient descent, and look whether the predicted class indeed moves towards the actual signal (decision threshold) that is present in the data. Note: this is indeed a bit of a silly exercise: we take random data, we pass it through a sigmoid, we say 'let's pretend that we have labels for this data, and the 30% highest values have label 1\" and now we fit to that. It is just to get you acquainted with the sigmoid and logistic regression. \n",
    "\n",
    "After looking at the images, answer the following questions: \n",
    "* Why is there still a cost when all predictions are correct? How is that possible?\n",
    "* What do you think the shape of an optimal decision boundary would look like in this case? In other words: what sort of shape would you \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = predictedClass[:, np.newaxis]\n",
    "values = vals[:, np.newaxis]\n",
    "thetas = np.array([[-5.0], [5.0]])\n",
    "alpha = 0.8\n",
    "stepsToTake = 1000\n",
    "thresholdXLocation = np.log(0.7/(1-0.7))\n",
    "\n",
    "\n",
    "# running gradient descent\n",
    "thetasAlongDescent = []\n",
    "thetasAlongDescent.append(thetas)\n",
    "costAlongDescent   = []\n",
    "for gradientDescentStep in range(0, stepsToTake):\n",
    "    costNow   = costFuncLogReg(values, labels, thetasAlongDescent[-1])\n",
    "    costAlongDescent.append(costNow)\n",
    "    newThetas = linAlgGradientDescent(values, labels, thetasAlongDescent[-1], alpha, cost_fun = \"logistic\")\n",
    "    thetasAlongDescent.append(newThetas)\n",
    "    \n",
    "#Plot the predictions for the untrained model and the trained model using a threshold of 0.5 (50%)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize = (10,10))\n",
    "\n",
    "initialPredictions = linAlgLogRegHypothesis(values, thetas)\n",
    "initialClassPred   = np.where(initialPredictions<=0.5,0,1)\n",
    "finalPredictions   = linAlgLogRegHypothesis(values, thetasAlongDescent[-1])\n",
    "finalClassPred     = np.where(finalPredictions<=0.5,0,1)\n",
    "sigmoidValues      = mySigmoid(values)\n",
    "\n",
    "ax[0].scatter(vals[np.where(np.ravel(initialClassPred) == 0)],\n",
    "              sigmoidValues[np.where(np.ravel(initialClassPred) == 0)],\n",
    "              color = \"darkred\", label = \"predicted negative class\")\n",
    "ax[0].scatter(vals[np.where(np.ravel(initialClassPred) == 1)],\n",
    "              sigmoidValues[np.where(np.ravel(initialClassPred) == 1)],\n",
    "              color = \"darkgreen\", label = \"predicted positive class\")\n",
    "ax[0].set_xlabel(\"Values\")\n",
    "ax[0].set_ylabel(\"Sigmoid output\")\n",
    "ax[0].vlines(thresholdXLocation, -0.2, 1.2, linestyle = \"dashed\", label = \"actual boundary class 0 and 1\")\n",
    "ax[0].set_ylim([0, 1])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[0].text(0.9, 0.2, 'Cost: ' + str(np.round(np.sum(costAlongDescent[0]), 2)), horizontalalignment='center', \n",
    "                                    verticalalignment='center', transform=ax[0].transAxes)\n",
    "ax[0].text(0.9, 0.1, 'Thetas: ' + str(np.ravel(thetas)), horizontalalignment='center', \n",
    "                                    verticalalignment='center', transform=ax[0].transAxes)\n",
    "\n",
    "ax[1].scatter(vals[np.where(np.ravel(finalClassPred) == 0)],\n",
    "              sigmoidValues[np.where(np.ravel(finalClassPred) == 0)],\n",
    "              color = \"darkred\", label = \"predicted negative class\")\n",
    "ax[1].scatter(vals[np.where(np.ravel(finalClassPred) == 1)],\n",
    "              sigmoidValues[np.where(np.ravel(finalClassPred) == 1)],\n",
    "              color = \"darkgreen\", label = \"predicted positive class\")\n",
    "ax[1].set_xlabel(\"Values\")\n",
    "ax[1].set_ylabel(\"Sigmoid output\")\n",
    "ax[1].vlines(thresholdXLocation, -0.2, 1.2, linestyle = \"dashed\", label = \"actual boundary class 0 and 1\")\n",
    "ax[1].set_ylim([0, 1])\n",
    "ax[1].legend()\n",
    "\n",
    "ax[1].text(0.9, 0.2, 'Cost: ' + str(np.round(np.sum(costAlongDescent[-1]), 2)), horizontalalignment='center',\n",
    "    verticalalignment='center', transform=ax[1].transAxes)\n",
    "ax[1].text(0.9, 0.1, 'Thetas: ' + str(np.ravel(np.round(thetasAlongDescent[-1], 2))), horizontalalignment='center', \n",
    "                                    verticalalignment='center', transform=ax[1].transAxes)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21bc24",
   "metadata": {},
   "source": [
    "## Plotting parameter changes along gradient descent\n",
    "Let's plot how the parameters of our linear regression change along gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70655446",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaTimeCourse = np.hstack(thetasAlongDescent)\n",
    "\n",
    "figTheta, axTheta = plt.subplots()\n",
    "axTheta.plot(range(0, stepsToTake), thetaTimeCourse[0,1:], label = \"Theta0\")\n",
    "axTheta.plot(range(0, stepsToTake), thetaTimeCourse[1,1:], label = \"Theta1\")\n",
    "\n",
    "print(\"Initial cost: \" + str(costAlongDescent[0]))\n",
    "print(\"Final cost: \" + str(costAlongDescent[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604238e-783d-45c8-be71-b76fcc0bf7b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca78c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d1d5e67",
   "metadata": {},
   "source": [
    "## Best decision boundary\n",
    "\n",
    "You can see that, in principle, the optimal situation (when we don't consider generalisation) is to simply parameterise the sigmoid function such that it becomes a so-called step function: <div>\n",
    "<img src=\"Dirac_distribution_CDF.svg\" width=\"500\"/>\n",
    "</div>\n",
    "at exactly the point where the class shifts from 0 to 1. You see that all the predictions before this threshold are (up to some numerical precision) 0, and all the ones after it are 1. To get this step function you need rather extreme values for $\\theta_0$ and $\\theta_1$. With gradient descent it would take ages to find these exact extreme values, as the gradients get very low in the extremes of the sigmoid function. With better optimizers like BFGS, it can be rather quick. Modern libraries use all sorts of tricks to improve numerical stability and get more well-behaved gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e4fa9",
   "metadata": {},
   "source": [
    "## Working with more features\n",
    "\n",
    "Until now we've worked with just one feature, which is somewhat unfulfilling. Let's change to two features and see what happens then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomFeat1, randomFeat2 = rng.standard_normal(100), rng.standard_normal(100)\n",
    "labels = np.logical_and(randomFeat1 <= 0.25,randomFeat2 <= 0.25).astype(int)\n",
    "\n",
    "plt.scatter(randomFeat1[labels == 0], randomFeat2[labels == 0], color = \"red\", label = \"negative samples\")\n",
    "plt.scatter(randomFeat1[labels == 1], randomFeat2[labels == 1], color = \"darkgreen\",\n",
    "            label = \"positive samples\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c6266",
   "metadata": {},
   "source": [
    "## Calculating the decision boundary\n",
    "We want to perform logistic regression for classification here. Below, I have provided code that uses scikit-learn's implementation of logistic regression. Full disclosure: I resorted to this due to numerical instability in other optimization algorithms. Anyway, we'll optimize logistic regression in a similar way that we implemented it, and then see what the final decision boundary is that is learned. You only need to get and plot the decision boundaries.\n",
    "\n",
    "Up to you to:\n",
    "* Plot the decision boundaries before and after optimisation (helpful code provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d860a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Prepare features\n",
    "features = np.hstack([randomFeat1[:, np.newaxis], randomFeat2[:, np.newaxis]])\n",
    "twoDLabels = labels[:, np.newaxis]\n",
    "\n",
    "# Define initial thetas for initial decision boundary\n",
    "initialThetas = np.array([0.2, 0.3, -0.1])\n",
    "\n",
    "###############\n",
    "#Ignore all this. Just optimizing logistic regression in scikit-learn and making a plot\n",
    "###############\n",
    "\n",
    "# Logistic regression hypothesis function (for making predictions)\n",
    "def linAlgLogRegHypothesis(x, thetas):\n",
    "    z = np.c_[np.ones(len(x)), x] @ thetas\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Train scikit-learn model with no regularization\n",
    "skl_model = LogisticRegression(penalty=None, fit_intercept=True, solver='lbfgs', max_iter=1000)\n",
    "skl_model.fit(features, labels)\n",
    "\n",
    "# Get scikit-learn parameters\n",
    "skl_intercept = skl_model.intercept_[0]\n",
    "skl_coef = skl_model.coef_[0]\n",
    "skl_thetas = np.array([skl_intercept, skl_coef[0], skl_coef[1]])\n",
    "\n",
    "# Get predictions from both models\n",
    "initial_probs = linAlgLogRegHypothesis(features, initialThetas)\n",
    "skl_probs = skl_model.predict_proba(features)[:, 1]\n",
    "\n",
    "# Calculate losses using scikit-learn's log_loss function\n",
    "initial_loss = log_loss(labels, initial_probs)\n",
    "skl_loss = log_loss(labels, skl_probs)\n",
    "\n",
    "print(\"Loss Comparison:\")\n",
    "print(f\"Initial parameters loss: {initial_loss:.4f}\")\n",
    "print(f\"Sklearn model loss: {skl_loss:.4f}\")\n",
    "print(f\"Improvement: {initial_loss - skl_loss:.4f} ({(1 - skl_loss/initial_loss)*100:.1f}%)\")\n",
    "\n",
    "# Calculate accuracy\n",
    "initial_preds = (initial_probs >= 0.5).astype(int)\n",
    "skl_preds = (skl_probs >= 0.5).astype(int)\n",
    "initial_accuracy = np.mean(initial_preds == labels) * 100\n",
    "skl_accuracy = np.mean(skl_preds == labels) * 100\n",
    "\n",
    "\n",
    "# Print the parameters comparison\n",
    "print(\"\\nParameter Comparison:\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Parameter':<12} | {'Initial Value':<15} | {'Optimized Value':<20}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Bias':<12} | {initialThetas[0]:<15.6f} | {skl_thetas[0]:<20.6f}\")\n",
    "print(f\"{'Weight1':<12} | {initialThetas[1]:<15.6f} | {skl_thetas[1]:<20.6f}\")\n",
    "print(f\"{'Weight2':<12} | {initialThetas[2]:<15.6f} | {skl_thetas[2]:<20.6f}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Loss':<12} | {initial_loss:<15.6f} | {skl_loss:<20.6f}\")\n",
    "print(f\"{'Accuracy':<12} | {initial_accuracy:<15.1f}% | {skl_accuracy:<20.1f}%\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# Create a grid for plotting\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Plot data and decision boundaries\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(randomFeat1[labels == 0], randomFeat2[labels == 0], \n",
    "            color=\"tomato\", label=\"Negative samples\", alpha=0.7, edgecolor='k')\n",
    "plt.scatter(randomFeat1[labels == 1], randomFeat2[labels == 1], \n",
    "            color=\"mediumseagreen\", label=\"Positive samples\", alpha=0.7, edgecolor='k')\n",
    "\n",
    "# Add text information\n",
    "plt.text(0.02, 0.98, f'Initial loss: {initial_loss:.4f} (accuracy: {initial_accuracy:.1f}%)', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top', \n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "plt.text(0.02, 0.93, f'Scikit-learn loss: {skl_loss:.4f} (accuracy: {skl_accuracy:.1f}%)', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "plt.text(0.02, 0.88, f'Loss improvement: {(1 - skl_loss/initial_loss)*100:.1f}%', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Enhance the plot appearance\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Logistic Regression: Initial vs. Optimized Decision Boundaries\")\n",
    "plt.ylim(-3, 3)\n",
    "plt.xlim(-3, 3)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "###############\n",
    "#You may stop ignoring now\n",
    "###############\n",
    "\n",
    "def getDecisionBoundaryYValues(feat1, thetas):\n",
    "    \"\"\"\n",
    "    Compute the y-values of the decision boundary line for logistic regression.\n",
    "\n",
    "    This function calculates the y-values of the decision boundary for a given \n",
    "    set of x-values (`feat1`) in a logistic regression model with two features. \n",
    "    The decision boundary is derived from the equation of a logistic regression model:\n",
    "\n",
    "        z = b + w1*x1 + w2*x2\n",
    "\n",
    "    The decision boundary is defined where the model outputs a probability of 0.5, \n",
    "    which corresponds to z = 0:\n",
    "\n",
    "        0 = b + w1*x1 + w2*x2\n",
    "\n",
    "    Solving for x2 (y-values):\n",
    "\n",
    "        x2 = (-w1 / w2) * x1 - (b / w2)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feat1 : array-like\n",
    "        The x-values (feature 1) used to compute corresponding decision boundary y-values.\n",
    "    thetas : tuple or list of floats\n",
    "        The model parameters (b, w1, w2), where:\n",
    "        - b  : Intercept term (bias)\n",
    "        - w1 : Weight for feature 1 (x-axis)\n",
    "        - w2 : Weight for feature 2 (y-axis)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    array-like\n",
    "        The computed y-values for the decision boundary.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - A small epsilon (1e-10) is added to `w2` if it's very close to zero to prevent division errors.\n",
    "    - This function assumes a linear decision boundary, applicable to logistic regression with two features.\n",
    "    \"\"\"\n",
    "    b, w1, w2 = thetas\n",
    "    # Avoid division by zero\n",
    "    if abs(w2) < 1e-10:\n",
    "        w2 = 1e-10\n",
    "    return (-w1 / w2) * feat1 - (b / w2)\n",
    "\n",
    "\n",
    "#Hint: use x_range\n",
    "#YOUR ANSWER HERE#\n",
    "\n",
    "\n",
    "\n",
    "#^ YOUR ANSWER THERE ^#\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87bcc8cf",
   "metadata": {},
   "source": [
    "## What I'd like you to remember here:\n",
    "* What the sigmoid function is and does, and why it is useful for classification.\n",
    "* How to plot basic graphs using matplotlib\n",
    "* How the cost function for logistic regression works (another name for it is the cross-entropy), and why we don't use the mean-squared error (MSE).\n",
    "* That there's more under the Sun than simple gradient descent: more advanced optimisation algorithms exist. You don't need to know how they work, but do need to know that they exist and you might want to use them. See [here](https://theaisummer.com/optimization/) and this great gif (which is from applying these algorithms to a logistic regression on the noisy moons dataset ([click for example](Noisy_Moons_example.png))): <br> <br> <br>\n",
    "![DiffOptimizers](optimisation_algorithms_NN.gif)\n",
    "\n",
    "## The end\n",
    "\n",
    "Congratulations, you've implemented logistic regression! The modern ML libraries you'll use when applying your knowledge to (biological) problems use appropriate optimisation methods behind the scenes, or if you need to select them manually you can simply test and see which works best. The thing you should remember is that if gradients get small, gradient descent can progress extremely slowly, and that normal stochastic gradient descent (SGD) can be outclassed by quite a few more advanced methods. Adam or RMSprop are often the standard in neural network optimisation, for instance.\n",
    "\n",
    "## Survey\n",
    "No worries, I see you over there, pining for that sweet, sweet survey. [I got you fam](https://docs.google.com/forms/d/e/1FAIpQLScoQynzU2aQrduUsmz8eimbE85Cn4_ytWJfnRTtEtcoHlLCaw/viewform?usp=sf_link)!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
